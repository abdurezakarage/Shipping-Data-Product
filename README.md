# Shipping Data Product - Telegram Analytics Pipeline

A comprehensive data pipeline for collecting, transforming, and analyzing Telegram channel data using modern data engineering practices.

##  Project Structure

```
Shipping-Data-Product/
â”œâ”€â”€ data/                          # Data lake
â”‚   â”œâ”€â”€ raw/                       # Raw JSON files
â”‚   â”‚   â””â”€â”€ telegrammessages/      # Partitioned by date/channel
â”‚   â””â”€â”€ labeled/                   # Processed data
â”œâ”€â”€ dbt_project/                   # DBT transformation project
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/              # Staging models
â”‚   â”‚   â””â”€â”€ marts/                # Data marts (star schema)
â”‚   â”œâ”€â”€ tests/                    # Custom data tests
â”‚   â”œâ”€â”€ macros/                   # Reusable SQL macros
â”‚   â””â”€â”€ profiles.yml              # Database connections
â”œâ”€â”€ src/                          # Python source code
â”‚   â”œâ”€â”€ load_raw_data.py          # Data loader script
â”‚   â””â”€â”€ telegram_scrapper.py      # Telegram scraper
â”œâ”€â”€ logs/                         # Application logs
â”œâ”€â”€ Dockerfile                    # Container configuration
â”œâ”€â”€ docker-compose.yml            # Multi-service setup
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .env                          # Environment variables
â””â”€â”€ README.md                     # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Docker and Docker Compose
- Git
- Python 3.11+ (for local development)

### 1. Clone and Setup

```bash
git clone <repository-url>
cd Shipping-Data-Product
```

### 2. Environment Configuration

Create a `.env` file with your configuration:

```bash
# Database Configuration
DB_HOST=localhost
DB_PORT=#
DB_NAME=shipping_data_warehouse
DB_USER=postgres
DB_PASSWORD=#

# DBT Configuration
DBT_PROJECT_NAME=shipping_dbt_project
DBT_PROFILE_NAME=shipping_profile

# Telegram Configuration
TELEGRAM_API_ID=#
TELEGRAM_API_HASH=#
TELEGRAM_PHONE=#

### 3. Start the Environment

```bash
# Start all services
docker-compose up -d

# Check service status
docker-compose ps
```

### 4. Load Raw Data

```bash
# Run the data loader
docker-compose exec data_pipeline python src/load_raw_data.py
```

### 5. Run DBT Transformations

```bash
# Access DBT container
docker-compose exec dbt bash

# Install DBT dependencies
dbt deps

# Run all models
dbt run


### Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Telegram      â”‚    â”‚   Data Lake     â”‚    â”‚   PostgreSQL    â”‚
â”‚   Channels      â”‚â”€â”€â”€â–¶â”‚   (csv Files)  â”‚â”€â”€â”€â–¶â”‚   (Raw Schema)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Analytics     â”‚â—€â”€â”€â”€â”‚   DBT Models    â”‚â—€â”€â”€â”€â”‚   Staging       â”‚
â”‚   Dashboard     â”‚    â”‚   (Star Schema) â”‚    â”‚   Models        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Star Schema Design

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   dim_dates     â”‚
                    â”‚   (Date Key)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ date_key
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dim_channels  â”‚    â”‚   fct_messages  â”‚    â”‚   dim_senders   â”‚
â”‚   (Channel Key) â”‚â”€â”€â”€â–¶â”‚   (Fact Table)  â”‚â—€â”€â”€â”€â”‚   (Sender Key)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                       â”‚
        â”‚ channel_key           â”‚ message_key           â”‚ sender_key
        â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Channel       â”‚    â”‚   Message       â”‚    â”‚   Sender        â”‚
â”‚   Analytics     â”‚    â”‚   Metrics       â”‚    â”‚   Analytics     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ DBT Models

### Staging Models

- **`stg_telegram_messages`**: Cleaned and standardized raw data
  - Data type casting and validation
  - Date/time extraction and formatting
  - Calculated fields (message length, word count)
  - Message type classification

## Analytical API (FastAPI)

This project includes a FastAPI-based analytical API for querying business metrics from your data warehouse.

### Install dependencies

```bash
pip install -r requirements.txt
```

### Run the API

From the project root:
```bash
uvicorn src.api.main:app --reload
```

- Visit [http://localhost:8000/docs](http://localhost:8000/docs) for interactive API documentation.
- Example endpoints:
  - `/api/reports/top-products?limit=10` â€” Most frequently mentioned products
  - `/api/channels/{channel_name}/activity` â€” Posting activity for a channel
  - `/api/search/messages?query=paracetamol` â€” Search messages by keyword


## âš™ï¸ Pipeline Orchestration (Dagster)

This project uses [Dagster](https://dagster.io/) for robust, observable, and schedulable data pipelines.

### Install Dagster

```bash
pip install dagster dagster-webserver
```

### Run the Dagster UI

From the project root:
```bash
export PYTHONPATH=.
# On Windows: set PYTHONPATH=.
dagster dev -f dagster_pipeline/repository.py
```

- Visit [http://localhost:3000](http://localhost:3000) to view, run, and monitor your pipeline.
- The pipeline includes these steps:
  1. `scrape_telegram_data` â€” Scrape Telegram messages
  2. `load_raw_to_postgres` â€” Load raw data into Postgres
  3. `run_dbt_transformations` â€” Run DBT models
  4. `run_yolo_enrichment` â€” Enrich data with YOLO object detection

 
